{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"In this work, I use torchtext library to preprocess and model IMDB reviews. Preprocessing includes tokenization, stemming and removal of stop words. Additionally, glove word vectors are used to improve performance. Finally, after obtaining word vectors, neural network based binary classifier is built.","metadata":{}},{"cell_type":"code","source":"import torch\nprint(torch.__version__) #version of the pytorh\nimport torch.nn.functional as F\n\n!pip install -U torch==1.8.0 torchtext==0.9.0\nimport torchtext.legacy as torchtext\nimport random\n\nimport nltk\n!pip install torchtext.data.BucketIterator\nfrom torchtext.legacy.data import BucketIterator\nnltk.download('punkt')\n\nnltk.download('punkt')\nnltk.download(\"stopwords\")\nfrom nltk.stem import SnowballStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import *","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:08:46.764275Z","iopub.execute_input":"2021-08-11T11:08:46.764609Z","iopub.status.idle":"2021-08-11T11:10:14.321664Z","shell.execute_reply.started":"2021-08-11T11:08:46.764580Z","shell.execute_reply":"2021-08-11T11:10:14.320705Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"1.7.0\nCollecting torch==1.8.0\n  Downloading torch-1.8.0-cp37-cp37m-manylinux1_x86_64.whl (735.5 MB)\n\u001b[K     |████████████████████████████████| 735.5 MB 10 kB/s s eta 0:00:01   |██▍                             | 55.2 MB 2.1 MB/s eta 0:05:23     |██▌                             | 56.7 MB 2.1 MB/s eta 0:05:23     |██▊                             | 63.4 MB 27.6 MB/s eta 0:00:25     |███▋                            | 84.2 MB 27.6 MB/s eta 0:00:24     |███▊                            | 85.9 MB 27.6 MB/s eta 0:00:24     |████▎                           | 99.3 MB 25.8 MB/s eta 0:00:25     |██████████▉                     | 249.7 MB 65.9 MB/s eta 0:00:08     |███████████                     | 251.2 MB 65.9 MB/s eta 0:00:08     |███████████                     | 252.8 MB 65.9 MB/s eta 0:00:08     |███████████▏                    | 256.5 MB 65.9 MB/s eta 0:00:08     |███████████▎                    | 258.1 MB 65.9 MB/s eta 0:00:08     |██████████████▎                 | 328.9 MB 51.7 MB/s eta 0:00:08     |█████████████████████▌          | 494.9 MB 52.5 MB/s eta 0:00:05\n\u001b[?25hCollecting torchtext==0.9.0\n  Downloading torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1 MB)\n\u001b[K     |████████████████████████████████| 7.1 MB 11.7 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.8.0) (3.7.4.3)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch==1.8.0) (1.19.5)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchtext==0.9.0) (2.25.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext==0.9.0) (4.61.1)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.9.0) (4.0.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.9.0) (2.10)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.9.0) (1.26.5)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.9.0) (2021.5.30)\nInstalling collected packages: torch, torchtext\n  Attempting uninstall: torch\n    Found existing installation: torch 1.7.0\n    Uninstalling torch-1.7.0:\n      Successfully uninstalled torch-1.7.0\n  Attempting uninstall: torchtext\n    Found existing installation: torchtext 0.8.0a0+cd6902d\n    Uninstalling torchtext-0.8.0a0+cd6902d:\n      Successfully uninstalled torchtext-0.8.0a0+cd6902d\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkornia 0.5.5 requires numpy<=1.19, but you have numpy 1.19.5 which is incompatible.\nfastai 2.2.7 requires torch<1.8,>=1.7.0, but you have torch 1.8.0 which is incompatible.\u001b[0m\nSuccessfully installed torch-1.8.0 torchtext-0.9.0\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n\u001b[31mERROR: Could not find a version that satisfies the requirement torchtext.data.BucketIterator (from versions: none)\u001b[0m\n\u001b[31mERROR: No matching distribution found for torchtext.data.BucketIterator\u001b[0m\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Preprocessing and Data Preperation**","metadata":{}},{"cell_type":"markdown","source":"Below, stemmer, stop word removal and tokenizer are defined. Then, those preprocessing steps are applied using data.field object.","metadata":{}},{"cell_type":"code","source":"stemmer = SnowballStemmer(\"english\")\nstop_words = stopwords.words(\"english\")\n\ndef tokenize_stem_stop(text): \n\n  nltk_tokens = nltk.word_tokenize(text)\n  stems = [stemmer.stem(token) for token in nltk_tokens]\n  filtered = []\n  for w in stems: \n    if w not in stop_words: \n        filtered.append(w) \n  return filtered\n  ","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:10:32.342444Z","iopub.execute_input":"2021-08-11T11:10:32.342858Z","iopub.status.idle":"2021-08-11T11:10:32.350477Z","shell.execute_reply.started":"2021-08-11T11:10:32.342821Z","shell.execute_reply":"2021-08-11T11:10:32.349606Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"TEXT = torchtext.data.Field(tokenize= tokenize_stem_stop,  batch_first=True) # preprocessing paraneters can be used to add aditional  preprocessing steps\nLABEL = torchtext.data.LabelField(dtype = torch.float)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:10:32.471546Z","iopub.execute_input":"2021-08-11T11:10:32.471838Z","iopub.status.idle":"2021-08-11T11:10:32.478199Z","shell.execute_reply.started":"2021-08-11T11:10:32.471809Z","shell.execute_reply":"2021-08-11T11:10:32.477320Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Dataset is downloaded from torchtext.datasets since it contains many large text sources for various applications.","metadata":{}},{"cell_type":"code","source":"train_data, test_data = torchtext.datasets.IMDB.splits(TEXT, LABEL)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:10:32.833506Z","iopub.execute_input":"2021-08-11T11:10:32.833794Z","iopub.status.idle":"2021-08-11T11:16:14.294364Z","shell.execute_reply.started":"2021-08-11T11:10:32.833766Z","shell.execute_reply":"2021-08-11T11:16:14.293395Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"aclImdb_v1.tar.gz:   0%|          | 0.00/84.1M [00:00<?, ?B/s]","output_type":"stream"},{"name":"stdout","text":"downloading aclImdb_v1.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:01<00:00, 45.1MB/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Below, maximum length are found to define fix number of words from each review.","metadata":{}},{"cell_type":"code","source":"max_size=0  ## this part of the code find maximum length of the network\ncount=0\nsum= 0\nfor i in  range(len(train_data)):\n  if max_size < len(train_data[i].text):\n    max_size =len(train_data[i].text)\n    print(max_size)\n  count +=1\n  sum +=len(train_data[0].text)\nprint(\"avarage: \", sum/count)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:16:14.295873Z","iopub.execute_input":"2021-08-11T11:16:14.296226Z","iopub.status.idle":"2021-08-11T11:16:14.333725Z","shell.execute_reply.started":"2021-08-11T11:16:14.296189Z","shell.execute_reply":"2021-08-11T11:16:14.332367Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"218\n653\n749\n1362\n1437\n1799\navarage:  218.0\n","output_type":"stream"}]},{"cell_type":"code","source":"TEXT = torchtext.data.Field(tokenize=tokenize_stem_stop, batch_first=True,fix_length= 1799\n                            \n                            ) # preprocessing parameters can be used to add aditional  preprocessing steps\nLABEL = torchtext.data.LabelField(dtype = torch.float)\ntrain_data, test_data = torchtext.datasets.IMDB.splits(TEXT, LABEL) \nprint(\"train length is: \",len(train_data))\nprint(\"test length is: \",len(test_data))\nprint(vars(train_data[0]))","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:16:14.336237Z","iopub.execute_input":"2021-08-11T11:16:14.336845Z","iopub.status.idle":"2021-08-11T11:21:29.154334Z","shell.execute_reply.started":"2021-08-11T11:16:14.336802Z","shell.execute_reply":"2021-08-11T11:21:29.153290Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"train length is:  25000\ntest length is:  25000\n{'text': ['feel', 'film', 'rare', 'treasur', '.', 'onli', 'begin', 'shirley', 'templ', \"'s\", 'career', ',', 'rare', 'look', 'societi', 'chang', '.', 'understand', ',', 'certain', 'thing', 'today', 'would', 'view', 'sexual', ',', 'back', 'would', 'consid', 'innoc', '.', 'exampl', ',', 'parent', 'children', 'film', 'well', 'mani', 'parent', 'took', 'children', 'see', 'movi', ',', 'saw', 'children', 'mimick', 'adult', '.', 'peopl', \"n't\", 'think', 'anyon', 'view', 'children', 'sexual', 'attract', ',', 'teenag', 'boy', 'lust', 'teenag', 'girl', '.', \"n't\", 'sexual', '.', 'mind', 'befor', 'internet', ',', 'tv', ',', 'etc', '...', 'sex', 'crime', \"n't\", 'open', 'brought', '.', 'occasion', 'would', 'whisper', 'kid', '``', 'funni', 'uncl', '.', \"''\", 'often', 'came', '.', 'yes', 'veri', 'sad', '.', 'kinda', 'sad', 'today', ',', 'even', 'see', 'film', 'anyth', 'intend', ',', 'innoc', 'funni', '.', 'saw', 'shirley', 'danc', 'like', 'boy', 'eye', 'ball', ',', 'yes', 'felt', 'disturb', '.', 'remind', 'time', 'took', 'place', '!', 'children', \"n't\", 'know', 'sex', '.', 'parent', 'knew', ',', 'children', 'movi', 'watch', '.', 'thought', 'may', 'even', 'enter', 'mind', '.', 'eye', 'averag', 'adult', 'back', ',', 'sexual', 'shirley', 'play', 'hous', '.', 'even', 'today', 'kid', 'enter', 'beauti', 'contest', ',', 'mani', 'dress', 'extrem', 'matur', ',', 'three', 'yr', 'old', '.', 'howev', 'child', 'mere', 'pretend', '.', \"n't\", 'blame', 'child', 'want', 'act', 'like', 'adult', '.', 'old', 'movi', 'display', '.', 'honesti', ',', 'media', 'made', 'lot', 'thing', 'seem', 'back', 'seem', 'sick', 'wrong', '.', 'sometim', 'best', '.', 'truli', 'believ', 'movi', \"n't\", 'one', '.', 'give', 'rare', 'look', 'innoc', 'mental', ',', 'long', 'lost', '.'], 'label': 'pos'}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"An example of preprocessed movie review can be seen. Lemmatization is more sophisticated approach then stemming however, it is more costly. ","metadata":{}},{"cell_type":"code","source":"TEXT.build_vocab(train_data, max_size = 20000,\n                 vectors=\"glove.6B.100d\",\n                 unk_init = torch.Tensor.normal_)\nLABEL.build_vocab(train_data)\nprint(\"Unique tokens in TEXT vocabulary:\",len(TEXT.vocab))\nprint(\"Unique tokens in LABEL vocabulary:\",len(LABEL.vocab))","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:21:29.156132Z","iopub.execute_input":"2021-08-11T11:21:29.156482Z","iopub.status.idle":"2021-08-11T11:24:52.138306Z","shell.execute_reply.started":"2021-08-11T11:21:29.156445Z","shell.execute_reply":"2021-08-11T11:24:52.137253Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":".vector_cache/glove.6B.zip: 862MB [02:40, 5.37MB/s]                               \n100%|█████████▉| 399999/400000 [00:20<00:00, 19646.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Unique tokens in TEXT vocabulary: 20002\nUnique tokens in LABEL vocabulary: 2\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Unique tokens in TEXT vocabulary:\",len(TEXT.vocab))\nprint(\"Unique tokens in LABEL vocabulary:\",len(LABEL.vocab))\nprint(TEXT.vocab.freqs.most_common(20))\nprint(LABEL.vocab.freqs)\nprint(TEXT.unk_token)\nprint(TEXT.pad_token)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:24:52.139768Z","iopub.execute_input":"2021-08-11T11:24:52.140303Z","iopub.status.idle":"2021-08-11T11:24:52.168474Z","shell.execute_reply.started":"2021-08-11T11:24:52.140263Z","shell.execute_reply":"2021-08-11T11:24:52.167384Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Unique tokens in TEXT vocabulary: 20002\nUnique tokens in LABEL vocabulary: 2\n[(',', 275887), ('.', 237172), ('/', 102097), ('>', 102036), ('<', 101971), ('br', 101871), (\"'s\", 62159), ('movi', 49987), ('film', 46624), (')', 36175), ('(', 35397), (\"n't\", 33376), (\"''\", 33138), ('``', 32786), ('one', 26807), ('!', 24560), ('like', 22243), ('?', 16088), ('time', 15225), ('good', 14871)]\nCounter({'pos': 12500, 'neg': 12500})\n<unk>\n<pad>\n","output_type":"stream"}]},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n##Create a train and test iterators using Bucket iterator method with batch size 32\ntrain_iterator, test_iterator = BucketIterator.splits((train_data,test_data),batch_size=64,device=device)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:24:52.170146Z","iopub.execute_input":"2021-08-11T11:24:52.170742Z","iopub.status.idle":"2021-08-11T11:24:52.227040Z","shell.execute_reply.started":"2021-08-11T11:24:52.170700Z","shell.execute_reply":"2021-08-11T11:24:52.226122Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"**Model**","metadata":{}},{"cell_type":"markdown","source":"The important part in modelling is the embedding stage. We can see that embedding dimensions, padding idx and num_embeddings are defined from previous steps.\n\nAfter, word embeddings are generated, these vectors are fed into 2 layer neural network for binary classification.","metadata":{}},{"cell_type":"code","source":"class Network(torch.nn.Module):\n    def __init__(self,pad_idx):\n        super().__init__()\n        self.embedding = torch.nn.Embedding(num_embeddings = 20002, embedding_dim =100,padding_idx = pad_idx)\n        self.layer1 = torch.nn.Linear(1799*100, 1000)\n        self.layer2 = torch.nn.Linear(1000, 1)\n\n\n    def forward(self, x):\n        x = self.embedding(x).view(x.size(0),-1)\n        x = self.layer1(x)\n        x = F.relu(x)\n        x = self.layer2(x)\n\n        return x       ","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:24:52.230341Z","iopub.execute_input":"2021-08-11T11:24:52.230645Z","iopub.status.idle":"2021-08-11T11:24:52.239615Z","shell.execute_reply.started":"2021-08-11T11:24:52.230585Z","shell.execute_reply":"2021-08-11T11:24:52.238605Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model = Network(pad_idx = TEXT.vocab.stoi[TEXT.pad_token])\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:24:52.242170Z","iopub.execute_input":"2021-08-11T11:24:52.242502Z","iopub.status.idle":"2021-08-11T11:24:53.644769Z","shell.execute_reply.started":"2021-08-11T11:24:52.242468Z","shell.execute_reply":"2021-08-11T11:24:53.643774Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Network(\n  (embedding): Embedding(20002, 100, padding_idx=1)\n  (layer1): Linear(in_features=179900, out_features=1000, bias=True)\n  (layer2): Linear(in_features=1000, out_features=1, bias=True)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"model.embedding.weight.data.copy_(TEXT.vocab.vectors)\nmodel.embedding.weight.data[TEXT.vocab.stoi[TEXT.unk_token]] = torch.zeros(100)\nmodel.embedding.weight.data[TEXT.vocab.stoi[TEXT.pad_token]] = torch.zeros(100)\nmodel.embedding.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:24:53.646443Z","iopub.execute_input":"2021-08-11T11:24:53.646796Z","iopub.status.idle":"2021-08-11T11:24:53.654392Z","shell.execute_reply.started":"2021-08-11T11:24:53.646758Z","shell.execute_reply":"2021-08-11T11:24:53.653334Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Choose a Loss function from torch.nn according to your network\nloss_fn = torch.nn.BCEWithLogitsLoss()\n\n#Define an Adam optimizer with learning rate 0.001 to optimize the parameters of our network\noptimizer =  torch.optim.Adam(params= model.parameters(),lr= 0.001)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:24:53.656146Z","iopub.execute_input":"2021-08-11T11:24:53.656500Z","iopub.status.idle":"2021-08-11T11:24:53.663703Z","shell.execute_reply.started":"2021-08-11T11:24:53.656461Z","shell.execute_reply":"2021-08-11T11:24:53.662875Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"model = model.to(device)\nloss_fn = loss_fn.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:24:53.664951Z","iopub.execute_input":"2021-08-11T11:24:53.665565Z","iopub.status.idle":"2021-08-11T11:24:58.129437Z","shell.execute_reply.started":"2021-08-11T11:24:53.665526Z","shell.execute_reply":"2021-08-11T11:24:58.128270Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"**Training**","metadata":{}},{"cell_type":"markdown","source":"Below, helper functions and training procedure are defined.","metadata":{}},{"cell_type":"code","source":"def accuracy_fn(predictions, labels):  \n\n  correct = (torch.round(torch.sigmoid(predictions)) == batch.label.squeeze(0)).float() \n  acc = correct.sum() / len(correct)\n  return acc","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:24:58.130710Z","iopub.execute_input":"2021-08-11T11:24:58.131055Z","iopub.status.idle":"2021-08-11T11:24:58.135322Z","shell.execute_reply.started":"2021-08-11T11:24:58.131001Z","shell.execute_reply":"2021-08-11T11:24:58.134543Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import time\n# Training loop\nN_EPOCHS = 2\n\ntr_loss = []\nmodel.train()\n\nfor epoch in range(N_EPOCHS):\n    \n    # Calculate training time\n    start_time = time.time()\n\n    epoch_loss = 0\n    epoch_acc = 0\n    batch_no = 0\n\n    \n    for batch in train_iterator:\n        \n        # Reset the gradient to not use them in multiple passes \n        optimizer.zero_grad()\n        \n        predictions = model(batch.text).squeeze(1)\n        loss = loss_fn(predictions, batch.label.squeeze(0))\n        \n        # Backprop\n        loss.backward()\n        \n        # Optimize the weights\n        optimizer.step()\n        \n        # Record accuracy and loss\n        epoch_loss += loss.item()\n\n        correct = (torch.round(torch.sigmoid(predictions)) == batch.label.squeeze(0)).float() \n        acc = correct.sum() / len(correct)\n        epoch_acc +=acc.item()\n\n        batch_no = batch_no +1\n        \n        if batch_no%60 == 0:\n          print(f'Epoch:  {epoch+1:2} | Batch No: {batch_no} | Loss: {loss.item():.3f} | Accuracy: {acc.item()*100:.2f}%')\n    \n    train_loss = epoch_loss / len(train_iterator)\n        \n    \n    end_time = time.time()\n\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    \n    # Save training metrics\n    tr_loss.append(train_loss)\n        \n    print(f'Epoch: {epoch+1:2} | Epoch Time: {elapsed_mins}m {elapsed_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} ')","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:24:58.136606Z","iopub.execute_input":"2021-08-11T11:24:58.137463Z","iopub.status.idle":"2021-08-11T11:25:48.724384Z","shell.execute_reply.started":"2021-08-11T11:24:58.137425Z","shell.execute_reply":"2021-08-11T11:25:48.722779Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Epoch:   1 | Batch No: 60 | Loss: 0.695 | Accuracy: 57.81%\nEpoch:   1 | Batch No: 120 | Loss: 0.611 | Accuracy: 64.06%\nEpoch:   1 | Batch No: 180 | Loss: 0.573 | Accuracy: 65.62%\nEpoch:   1 | Batch No: 240 | Loss: 0.423 | Accuracy: 84.38%\nEpoch:   1 | Batch No: 300 | Loss: 0.493 | Accuracy: 78.12%\nEpoch:   1 | Batch No: 360 | Loss: 0.390 | Accuracy: 79.69%\nEpoch:  1 | Epoch Time: 0m 25s\n\tTrain Loss: 0.570 \nEpoch:   2 | Batch No: 60 | Loss: 0.230 | Accuracy: 90.62%\nEpoch:   2 | Batch No: 120 | Loss: 0.189 | Accuracy: 92.19%\nEpoch:   2 | Batch No: 180 | Loss: 0.136 | Accuracy: 95.31%\nEpoch:   2 | Batch No: 240 | Loss: 0.229 | Accuracy: 90.62%\nEpoch:   2 | Batch No: 300 | Loss: 0.270 | Accuracy: 87.50%\nEpoch:   2 | Batch No: 360 | Loss: 0.153 | Accuracy: 92.19%\nEpoch:  2 | Epoch Time: 0m 24s\n\tTrain Loss: 0.211 \n","output_type":"stream"}]},{"cell_type":"code","source":"test_epoch_loss = 0\ntest_epoch_acc = 0\n\n# Turm on evalutaion mode\nmodel.eval()\n\n# No need to backprop in eval\nwith torch.no_grad():\n\n    for batch in test_iterator:\n\n        test_predictions = model(batch.text).squeeze(1)\n        \n        test_loss = loss_fn(test_predictions, batch.label)\n\n        test_epoch_loss += test_loss.item()\n        \n        acc = accuracy_fn(test_predictions,batch.label.squeeze(0))\n        test_epoch_acc +=acc.item()\n\ntest_loss = test_epoch_loss/len(test_iterator)\ntest_acc = test_epoch_acc  / len(test_iterator)\nprint(f'Test Loss: {test_loss:.3f} | | Test Acc: {test_acc*100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2021-08-11T11:25:48.725681Z","iopub.execute_input":"2021-08-11T11:25:48.726010Z","iopub.status.idle":"2021-08-11T11:25:57.998876Z","shell.execute_reply.started":"2021-08-11T11:25:48.725975Z","shell.execute_reply":"2021-08-11T11:25:57.998042Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Test Loss: 0.453 | | Test Acc: 81.18%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"After several trails, I see that GloVe pretrained word vectors improve model performance significantly. Also, preprocessing steps such as stop word removal and stemming improve overall scores.\n\nAt  the end, 81.18% is a nice test accuracy score to classify good and bad reviews, using IMDB review dataset.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}