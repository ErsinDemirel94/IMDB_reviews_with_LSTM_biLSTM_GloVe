{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**LSTM and bi-LSTM Model for IMDB Sentiment Classification**","metadata":{}},{"cell_type":"markdown","source":"First, required modules are imported. For this task, torch, torchtext and nltk libraries are used.","metadata":{}},{"cell_type":"code","source":"import torch\nprint(torch.__version__) \nimport torch.nn.functional as F\nimport torch.nn as nn\n!pip install -U torch==1.8.0 torchtext==0.9.0\nimport torchtext.legacy as torchtext\nimport random\n!pip install contractions\nimport contractions\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem import SnowballStemmer","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-12T11:15:07.486433Z","iopub.execute_input":"2021-08-12T11:15:07.486931Z","iopub.status.idle":"2021-08-12T11:15:20.545772Z","shell.execute_reply.started":"2021-08-12T11:15:07.486887Z","shell.execute_reply":"2021-08-12T11:15:20.544755Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"1.7.0\nRequirement already satisfied: torch==1.8.0 in /opt/conda/lib/python3.7/site-packages (1.8.0)\nRequirement already satisfied: torchtext==0.9.0 in /opt/conda/lib/python3.7/site-packages (0.9.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torch==1.8.0) (1.19.5)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch==1.8.0) (3.7.4.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from torchtext==0.9.0) (4.61.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchtext==0.9.0) (2.25.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.9.0) (1.26.5)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.9.0) (2.10)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.9.0) (2021.5.30)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->torchtext==0.9.0) (4.0.0)\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\nRequirement already satisfied: contractions in /opt/conda/lib/python3.7/site-packages (0.0.52)\nRequirement already satisfied: textsearch>=0.0.21 in /opt/conda/lib/python3.7/site-packages (from contractions) (0.0.21)\nRequirement already satisfied: anyascii in /opt/conda/lib/python3.7/site-packages (from textsearch>=0.0.21->contractions) (0.2.0)\nRequirement already satisfied: pyahocorasick in /opt/conda/lib/python3.7/site-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Then, preprocessing function is defined. In this code, I used contradictions, which fixes contractions such as you're to you are. And, I applied tweet tokenizer to tokenize string input. Finally, stemmer function applied to normalize words into their roots roughly. Then, using these fields, IMDB dataset is processed and splitted into training and test. Lengths are also saved for TEXT field.","metadata":{}},{"cell_type":"code","source":"tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True)\nstemmer = SnowballStemmer(\"english\")\n\ndef tokenize_fn(text):\n  text = contractions.fix(text)\n  tokens = tokenizer.tokenize(text)\n  tokens = [stemmer.stem(token) for token in tokens]\n  return tokens\n\nstop_words = [\"and\",\".\",\",\",\"the\",\"a\",\"of\",\"is\",\"to\",\"/\",\"it\",\">\",\"<\",\"br\",\"(\",\")\",\"this\",\"that\",\"was\",\"for\",\"with\",\"have\",\"be\"] # most frequent words and punctuations are dropped.\n\nTEXT = torchtext.data.Field(tokenize = tokenize_fn, lower = True, stop_words=stop_words, include_lengths=True) # length of sequences are included.\nLABEL = torchtext.data.LabelField(dtype = torch.float)\n\ntrain_data, test_data = torchtext.datasets.IMDB.splits(TEXT, LABEL) ","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:15:20.547632Z","iopub.execute_input":"2021-08-12T11:15:20.548000Z","iopub.status.idle":"2021-08-12T11:19:44.658823Z","shell.execute_reply.started":"2021-08-12T11:15:20.547958Z","shell.execute_reply":"2021-08-12T11:19:44.657866Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"Below, number of examples and 2 examples in detail are shown. We can see that input texts are in arbitrary length. Also, we can see that words are normalized into their roots.","metadata":{}},{"cell_type":"code","source":"print(f\"Number of training examples: {len(train_data.examples)}\")\nprint(f\"Number of testing examples: {len(test_data.examples)}\")\n","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:19:44.660825Z","iopub.execute_input":"2021-08-12T11:19:44.661187Z","iopub.status.idle":"2021-08-12T11:19:44.667314Z","shell.execute_reply.started":"2021-08-12T11:19:44.661147Z","shell.execute_reply":"2021-08-12T11:19:44.666430Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Number of training examples: 25000\nNumber of testing examples: 25000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Validation set is also created with splitting training data randomly. Below, train/test/val ratios can be seen.","metadata":{}},{"cell_type":"code","source":"import random\n\ntrain_data, valid_data = train_data.split(random_state = random.seed(42))\n\nprint(f'Number of training examples: {len(train_data)}')\nprint(f'Number of validation examples: {len(valid_data)}')\nprint(f'Number of testing examples: {len(test_data)}')","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:19:44.669088Z","iopub.execute_input":"2021-08-12T11:19:44.669665Z","iopub.status.idle":"2021-08-12T11:19:44.722606Z","shell.execute_reply.started":"2021-08-12T11:19:44.669622Z","shell.execute_reply":"2021-08-12T11:19:44.721702Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Number of training examples: 17500\nNumber of validation examples: 7500\nNumber of testing examples: 25000\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Below, vocabularies are created. We can see frequency of each instance. Additionally, string to integer (stoi), and integer to string(itos) conversions are provided. Notice that even though we set max_size 17000, number of unique tokens are 17002. It is because including 2 tokens; <\\unk> and <\\pad>.","metadata":{}},{"cell_type":"code","source":"TEXT.build_vocab(train_data,min_freq=5,max_size = 17000)\nLABEL.build_vocab(train_data)\n\nprint(f\"Unique tokens in source vocabulary: {len(TEXT.vocab)}\")\nprint(f\"Unique tokens in TRG vocabulary: {len(LABEL.vocab)}\")","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:19:44.724691Z","iopub.execute_input":"2021-08-12T11:19:44.725044Z","iopub.status.idle":"2021-08-12T11:19:45.519034Z","shell.execute_reply.started":"2021-08-12T11:19:44.725007Z","shell.execute_reply":"2021-08-12T11:19:45.517838Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"Unique tokens in source vocabulary: 17002\nUnique tokens in TRG vocabulary: 2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Below, training, validation and test iterators created by bucket iterator. It is important to use this module because since our input sequences has arbitrary lengths, this method creates batches with close lengths.","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 64\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntrain_iterator, valid_iterator, test_iterator = torchtext.data.BucketIterator.splits(\n    (train_data, valid_data, test_data), \n    batch_size = BATCH_SIZE,\n    sort_within_batch = True,\n    device = device)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:19:45.520724Z","iopub.execute_input":"2021-08-12T11:19:45.521140Z","iopub.status.idle":"2021-08-12T11:19:45.803909Z","shell.execute_reply.started":"2021-08-12T11:19:45.521086Z","shell.execute_reply":"2021-08-12T11:19:45.803030Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"**Model**","metadata":{}},{"cell_type":"markdown","source":"Finally, model is constructed below. Network consists of an embedding layer, a LSTM layer and one fully connected layer with dropouts. I tried to initialize hidden and cell states, but I couldn't fit shapes of tensors, thus, I use default mode.","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass LSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n                 dropout, pad_idx):\n        \n        super().__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n        \n        self.rnn = nn.LSTM(embedding_dim, \n                           hidden_dim, \n                           num_layers=n_layers, \n\n                           dropout=dropout)\n        \n        self.fc = nn.Linear(hidden_dim , output_dim) \n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, text, text_lengths):\n        #h0 = torch.zeros((n_layers, BATCH_SIZE, hidden_dim)).to(device)\n        #c0 = torch.zeros((n_layers, BATCH_SIZE, hidden_dim)).to(device)  \n        embedded = self.dropout(self.embedding(text))\n\n        #output, (hidden, cell) = self.rnn(embedded,(h0,c0))\n        output, (hidden, cell) = self.rnn(embedded)\n\n        #output = [sent len, batch size, hid dim * num directions]        \n        hidden = self.dropout(output)[-1,:,:]\n                         \n        return self.fc(hidden)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:19:45.807779Z","iopub.execute_input":"2021-08-12T11:19:45.808062Z","iopub.status.idle":"2021-08-12T11:19:45.815599Z","shell.execute_reply.started":"2021-08-12T11:19:45.808034Z","shell.execute_reply":"2021-08-12T11:19:45.814591Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"input_dim = len(TEXT.vocab)\noutput_dim = 1\nembedding_dim = 200\nhidden_dim = 100\nn_layers = 1\ndropout = 0.5\npad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n\nmodel = LSTM(input_dim, \n            embedding_dim, \n            hidden_dim, \n            output_dim, \n            n_layers,  \n            dropout, \n            pad_idx)\n\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:19:45.819030Z","iopub.execute_input":"2021-08-12T11:19:45.819560Z","iopub.status.idle":"2021-08-12T11:19:45.875438Z","shell.execute_reply.started":"2021-08-12T11:19:45.819521Z","shell.execute_reply":"2021-08-12T11:19:45.874292Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"LSTM(\n  (embedding): Embedding(17002, 200, padding_idx=1)\n  (rnn): LSTM(200, 100, dropout=0.5)\n  (fc): Linear(in_features=100, out_features=1, bias=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.optim as optim\n\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\ncriterion = nn.BCEWithLogitsLoss()\nmodel = model.to(device)\ncriterion = criterion.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:19:45.877417Z","iopub.execute_input":"2021-08-12T11:19:45.877821Z","iopub.status.idle":"2021-08-12T11:19:45.888910Z","shell.execute_reply.started":"2021-08-12T11:19:45.877777Z","shell.execute_reply":"2021-08-12T11:19:45.888066Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"After defining custom functions for accuracy, training, evaluation and calculating epoch time, network is trained and, results are printed. I also tried training with more epochs, and see that accuracy is increasing over epochs.","metadata":{}},{"cell_type":"code","source":"def binary_accuracy(preds, y):\n\n    #round predictions to the closest integer\n    rounded_preds = torch.round(torch.sigmoid(preds))\n    correct = (rounded_preds == y).float() #convert into float for division \n    acc = correct.sum() / len(correct)\n    return acc","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:19:45.890721Z","iopub.execute_input":"2021-08-12T11:19:45.891398Z","iopub.status.idle":"2021-08-12T11:19:45.896986Z","shell.execute_reply.started":"2021-08-12T11:19:45.891359Z","shell.execute_reply":"2021-08-12T11:19:45.895951Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"def train(model, iterator, optimizer, criterion):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.train()\n    \n    for batch in iterator:\n        \n        optimizer.zero_grad()        \n        text, text_lengths = batch.text        \n        predictions = model(text, text_lengths).squeeze(1)        \n        loss = criterion(predictions, batch.label)        \n        acc = binary_accuracy(predictions, batch.label)        \n        loss.backward()        \n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n    return epoch_loss / len(iterator), epoch_acc / len(iterator)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:19:45.898679Z","iopub.execute_input":"2021-08-12T11:19:45.899093Z","iopub.status.idle":"2021-08-12T11:19:45.908990Z","shell.execute_reply.started":"2021-08-12T11:19:45.899008Z","shell.execute_reply":"2021-08-12T11:19:45.907848Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, iterator, criterion):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.eval()\n    \n    with torch.no_grad():\n    \n        for batch in iterator:\n\n            text, text_lengths = batch.text    \n            predictions = model(text, text_lengths).squeeze(1)           \n            loss = criterion(predictions, batch.label)            \n            acc = binary_accuracy(predictions, batch.label)\n\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n        \n    return epoch_loss / len(iterator), epoch_acc / len(iterator)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:19:45.912082Z","iopub.execute_input":"2021-08-12T11:19:45.912402Z","iopub.status.idle":"2021-08-12T11:19:45.922496Z","shell.execute_reply.started":"2021-08-12T11:19:45.912375Z","shell.execute_reply":"2021-08-12T11:19:45.921474Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"import time\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:19:45.923879Z","iopub.execute_input":"2021-08-12T11:19:45.924332Z","iopub.status.idle":"2021-08-12T11:19:45.932092Z","shell.execute_reply.started":"2021-08-12T11:19:45.924282Z","shell.execute_reply":"2021-08-12T11:19:45.931095Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"N_EPOCHS = 5\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n\n    start_time = time.time()\n    \n    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n    \n    end_time = time.time()\n\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n           \n    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:19:45.933680Z","iopub.execute_input":"2021-08-12T11:19:45.934108Z","iopub.status.idle":"2021-08-12T11:20:13.071872Z","shell.execute_reply.started":"2021-08-12T11:19:45.934071Z","shell.execute_reply":"2021-08-12T11:20:13.070421Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"Epoch: 01 | Epoch Time: 0m 5s\n\tTrain Loss: 0.669 | Train Acc: 58.58%\n\t Val. Loss: 0.588 |  Val. Acc: 70.55%\nEpoch: 02 | Epoch Time: 0m 5s\n\tTrain Loss: 0.613 | Train Acc: 67.46%\n\t Val. Loss: 0.608 |  Val. Acc: 72.01%\nEpoch: 03 | Epoch Time: 0m 5s\n\tTrain Loss: 0.570 | Train Acc: 72.92%\n\t Val. Loss: 0.568 |  Val. Acc: 73.45%\nEpoch: 04 | Epoch Time: 0m 5s\n\tTrain Loss: 0.528 | Train Acc: 76.04%\n\t Val. Loss: 0.574 |  Val. Acc: 74.06%\nEpoch: 05 | Epoch Time: 0m 5s\n\tTrain Loss: 0.510 | Train Acc: 77.41%\n\t Val. Loss: 0.575 |  Val. Acc: 74.69%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**bi-LSTM Model**","metadata":{}},{"cell_type":"markdown","source":"Model is constructed below. It is same as LSTM (embedding/LSTM/FC and dropouts), however in this part, in lstm layer, bidirectional=True. Also, number of hidden layers are multiplied by 2 since bilstm has 2 lstm layers, one for forward sweep, and one for backward.\n\nEmbeddings are packed with nn.utils.rnn.packed_padded_sequence before feeding to biLSTM layer. This enable biLSTM to only process non-padded elements in sequence. After processing through lstm, sequence is unpacked with nn.utils.rnn.pad_packed_sequence, to transform them back to tensors.\n\nAdditionally, since we have 2 hidden layers (backward, forward) we finally concatenate them into one single hidden layer output.","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\n\nclass biLSTM(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n                 dropout, pad_idx):\n        \n        super().__init__()\n        \n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n        \n        self.rnn = nn.LSTM(embedding_dim, \n                           hidden_dim, \n                           num_layers=n_layers, \n                           bidirectional=True, # bidirectional lstm\n                           dropout=dropout)\n        \n        self.fc = nn.Linear(hidden_dim * 2, output_dim) # *2 because 2 lstms, fwd and bwd.\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, text, text_lengths):\n           \n        embedded = self.dropout(self.embedding(text))\n\n        #pack sequence\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.to('cpu'))       \n        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n        \n        #unpack sequence\n        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n\n        #output = [sent len, batch size, hid dim * num directions]        \n        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n                         \n        return self.fc(hidden)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:20:13.073371Z","iopub.execute_input":"2021-08-12T11:20:13.073718Z","iopub.status.idle":"2021-08-12T11:20:13.082974Z","shell.execute_reply.started":"2021-08-12T11:20:13.073682Z","shell.execute_reply":"2021-08-12T11:20:13.082003Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"input_dim = len(TEXT.vocab)\noutput_dim = 1\nembedding_dim = 200\nhidden_dim = 100\nn_layers = 1\ndropout = 0.5\npad_idx = TEXT.vocab.stoi[TEXT.pad_token]\n\nbi_model = biLSTM(input_dim, \n            embedding_dim, \n            hidden_dim, \n            output_dim, \n            n_layers,  \n            dropout, \n            pad_idx)\n\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:20:13.084351Z","iopub.execute_input":"2021-08-12T11:20:13.084820Z","iopub.status.idle":"2021-08-12T11:20:13.132629Z","shell.execute_reply.started":"2021-08-12T11:20:13.084783Z","shell.execute_reply":"2021-08-12T11:20:13.131847Z"},"trusted":true},"execution_count":44,"outputs":[{"name":"stdout","text":"LSTM(\n  (embedding): Embedding(17002, 200, padding_idx=1)\n  (rnn): LSTM(200, 100, dropout=0.5)\n  (fc): Linear(in_features=100, out_features=1, bias=True)\n  (dropout): Dropout(p=0.5, inplace=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch.optim as optim\n\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\n\ncriterion = nn.BCEWithLogitsLoss()\nmodel = model.to(device)\ncriterion = criterion.to(device)\n\nN_EPOCHS = 5\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n\n    start_time = time.time()\n    \n    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n    \n    end_time = time.time()\n\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n           \n    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2021-08-12T11:20:13.133792Z","iopub.execute_input":"2021-08-12T11:20:13.134119Z","iopub.status.idle":"2021-08-12T11:20:41.481725Z","shell.execute_reply.started":"2021-08-12T11:20:13.134085Z","shell.execute_reply":"2021-08-12T11:20:41.479918Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stdout","text":"Epoch: 01 | Epoch Time: 0m 5s\n\tTrain Loss: 0.462 | Train Acc: 80.46%\n\t Val. Loss: 0.614 |  Val. Acc: 72.90%\nEpoch: 02 | Epoch Time: 0m 5s\n\tTrain Loss: 0.458 | Train Acc: 79.63%\n\t Val. Loss: 0.493 |  Val. Acc: 79.56%\nEpoch: 03 | Epoch Time: 0m 5s\n\tTrain Loss: 0.452 | Train Acc: 79.66%\n\t Val. Loss: 0.594 |  Val. Acc: 77.13%\nEpoch: 04 | Epoch Time: 0m 5s\n\tTrain Loss: 0.395 | Train Acc: 83.46%\n\t Val. Loss: 0.454 |  Val. Acc: 79.64%\nEpoch: 05 | Epoch Time: 0m 5s\n\tTrain Loss: 0.361 | Train Acc: 85.01%\n\t Val. Loss: 0.377 |  Val. Acc: 84.67%\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We can see that bi-lstm model gave significantly better accuracy after 5 epochs. LSTM: 74.66%, bi-LSTM: 84.67%","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}